{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import DihedralAdherence\n",
    "from lib import PDBMineQuery, MultiWindowQuery\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lib.constants import AMINO_ACID_MAP, AMINO_ACID_MAP_INV\n",
    "PDBMINE_URL = os.getenv(\"PDBMINE_URL\")\n",
    "PROJECT_DIR = 'ml_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results already exist\n",
      "Structure exists: 'pdb/pdb8dnv.ent' \n",
      "(55, 2) 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 24.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:02, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches for 8DNV\n",
      "Results already exist\n",
      "Structure exists: 'pdb/pdb6beu.ent' \n",
      "Results already exist\n",
      "Structure exists: 'pdb/pdb1j1o.ent' \n",
      "(101, 2) 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:01, 57.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches for 1J1O\n",
      "           4      5      6      7\n",
      "count  156.0  156.0  156.0  156.0\n",
      "mean     0.0    0.0    0.0    0.0\n",
      "std      0.0    0.0    0.0    0.0\n",
      "min      0.0    0.0    0.0    0.0\n",
      "25%      0.0    0.0    0.0    0.0\n",
      "50%      0.0    0.0    0.0    0.0\n",
      "75%      0.0    0.0    0.0    0.0\n",
      "max      0.0    0.0    0.0    0.0\n",
      "4    0.0\n",
      "5    0.0\n",
      "6    0.0\n",
      "7    0.0\n",
      "Name: 0.95, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PDBMINE_URL = os.getenv(\"PDBMINE_URL\")\n",
    "PROJECT_DIR = 'ml_data'\n",
    "# casp_protein_ids = ['T1024', 'T1096', 'T1027', 'T1082', 'T1091', 'T1058', 'T1049', 'T1030', 'T1056', 'T1038', 'T1025', 'T1028']\n",
    "# pdb_codes = ['6T1Z', '7UM1', '7D2O', '7CN6', '7W6B', '7ABW', '6Y4F', '6POO', '6YJ1', '6YA2', '6UV6', '6VQP']\n",
    "pdb_codes = [f.name.split('_')[0] for f in Path(PROJECT_DIR).iterdir() if f.is_dir() and len(list(f.iterdir())) == 5]\n",
    "winsizes = [4,5,6,7]\n",
    "lengths = [4096, 512, 256, 256]\n",
    "lengths_dict = {w:l for w,l in zip(winsizes, lengths)}\n",
    "\n",
    "outdir = Path('ml_samples/' + '-'.join(str(l) for l in lengths))\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "all_seqs = []\n",
    "n_matches = defaultdict(list)\n",
    "for id in pdb_codes:\n",
    "    if (outdir / f'{id}.pt').exists():\n",
    "        continue\n",
    "    da = MultiWindowQuery(id, winsizes, PDBMINE_URL, PROJECT_DIR)\n",
    "    da.load_results()\n",
    "    xray = da.xray_phi_psi\n",
    "    seqs = xray[['seq_ctxt', 'res']].drop_duplicates().rename(columns={'seq_ctxt': 'seq'})\n",
    "    seqs = seqs[~seqs.seq.str.contains('X')]\n",
    "    if seqs.shape[0] == 0:\n",
    "        continue\n",
    "    all_seqs.append(seqs)\n",
    "\n",
    "    print(seqs.shape, seqs.seq.nunique())\n",
    "    X = []\n",
    "    y = []\n",
    "    x_res = []\n",
    "    for i,row in tqdm(seqs.iterrows()):\n",
    "        phis = []\n",
    "        psis = []\n",
    "        xray_phi = xray[xray.seq_ctxt == row.seq].phi.values[0]\n",
    "        xray_psi = xray[xray.seq_ctxt == row.seq].psi.values[0]\n",
    "        if np.isnan(xray_phi) or np.isnan(xray_psi):\n",
    "            for w in winsizes:\n",
    "                n_matches[w].append(0)\n",
    "            continue\n",
    "        for q in da.queries:\n",
    "            inner_seq = q.get_subseq(row.seq)\n",
    "            matches = q.results[q.results.seq == inner_seq][['seq', 'phi', 'psi']]\n",
    "            n_matches[q.winsize].append(matches.shape[0])\n",
    "            if matches.shape[0] == 0:\n",
    "                phis.append(np.zeros(lengths_dict[q.winsize]))\n",
    "                psis.append(np.zeros(lengths_dict[q.winsize]))\n",
    "                continue\n",
    "            phi = matches.phi.values\n",
    "            psi = matches.psi.values\n",
    "            if matches.shape[0] >= lengths_dict[q.winsize]:\n",
    "                phi = np.random.choice(phi, lengths_dict[q.winsize], replace=False)\n",
    "                psi = np.random.choice(psi, lengths_dict[q.winsize], replace=False)\n",
    "            else:\n",
    "                phi = np.pad(phi, (0, lengths_dict[q.winsize] - matches.shape[0]))\n",
    "                psi = np.pad(psi, (0, lengths_dict[q.winsize] - matches.shape[0]))\n",
    "            phis.append(phi)\n",
    "            psis.append(psi)\n",
    "        # if len(phis) == 0:\n",
    "            # continue\n",
    "        phis = np.concatenate(phis)\n",
    "        psis = np.concatenate(psis)\n",
    "        if np.sum(phis) == 0: # no matches\n",
    "            continue\n",
    "        X.append(np.stack([phis, psis]))\n",
    "        y.append(np.array([xray_phi, xray_psi]))\n",
    "        x_res.append(AMINO_ACID_MAP[row.res])\n",
    "    if len(X) == 0:\n",
    "        print('No matches for', id)\n",
    "        continue\n",
    "    X = np.stack(X)\n",
    "    y = np.stack(y)\n",
    "    x_res = F.one_hot(torch.Tensor(x_res).to(torch.int64), num_classes=20)\n",
    "    torch.save((torch.Tensor(X), torch.Tensor(y), x_res), outdir / f'{id}.pt')\n",
    "all_seqs = pd.concat(all_seqs)\n",
    "for k,v in n_matches.items():\n",
    "    all_seqs[k] = v\n",
    "print(all_seqs.describe())\n",
    "print(all_seqs[winsizes].quantile(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein_cache = {}\n",
    "# max_proteins = 100\n",
    "\n",
    "# class ProteinDataset(Dataset):\n",
    "#     def __init__(self, id, path):\n",
    "#         self.id = id\n",
    "#         self.path = path\n",
    "#         X, _, _ = torch.load(self.path / f'{self.id}.pt')\n",
    "#         self.shape = X.shape\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.shape[0]\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         if self.id in protein_cache:\n",
    "#             X, y, xres = protein_cache[self.id]\n",
    "#         else:\n",
    "#             X, y, xres = torch.load(self.path / f'{self.id}.pt')\n",
    "#             if len(protein_cache) < max_proteins:\n",
    "#                 protein_cache[self.id] = (X, y, xres)\n",
    "#             else:\n",
    "#                 del protein_cache[list(protein_cache.keys())[0]]\n",
    "#                 protein_cache[self.id] = (X, y, xres)\n",
    "#         return X[i], xres[i], y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, id, path):\n",
    "        self.id = id\n",
    "        self.path = path\n",
    "\n",
    "        self.X, self.y, self.xres = torch.load(self.path / f'{id}.pt')\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.xres[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.retrieve_data import retrieve_target_list\n",
    "ids = ['T1024', 'T1096', 'T1027', 'T1082', 'T1091', 'T1058', 'T1049', 'T1030', 'T1056', 'T1038', 'T1025', 'T1028']\n",
    "targetlist = retrieve_target_list()\n",
    "skip = [targetlist.loc[id, 'pdb_code'].upper() for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56926, 32834, 89760)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [4096, 512, 256, 256]\n",
    "path = Path('ml_samples/'+'-'.join([str(l) for l in lengths]))\n",
    "samples = [f.stem for f in path.iterdir()]\n",
    "samples = sorted(list(set(samples) - set(skip)))\n",
    "train, test = train_test_split(samples, test_size=0.35, random_state=42)\n",
    "torch.save((train, test), 'ml_data/split.pt')\n",
    "# train, test = to ch.load('ml_data/split.pt')\n",
    "train_dataset = ConcatDataset([ProteinDataset(s, path) for s in train])\n",
    "test_dataset = ConcatDataset([ProteinDataset(s, path) for s in test])\n",
    "trainloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "len(train_dataset), len(test_dataset), len(train_dataset) + len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,xres,y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(Xp, y, i, logits=None, logits2=None, res=None):\n",
    "    ls = lengths\n",
    "    Xp = Xp.cpu().clone().detach()\n",
    "    y = y.cpu().clone().detach()\n",
    "    Xp[Xp==0] = np.nan\n",
    "    s = [sum(lengths[:i]) for i,l in enumerate(ls)]\n",
    "    s = [sum(lengths[:i]) for i,l in enumerate(lengths)]\n",
    "    plt.plot(Xp[i, 0, s[0]:s[1]], Xp[i, 1, s[0]:s[1]], 'o', label='4')\n",
    "    plt.plot(Xp[i, 0, s[1]:s[2]], Xp[i, 1, s[1]:s[2]], 'o', label='5')\n",
    "    plt.plot(Xp[i, 0, s[2]:s[3]], Xp[i, 1, s[2]:s[3]], 'o', label='6')\n",
    "    plt.plot(Xp[i, 0, s[3]:    ], Xp[i, 1, s[3]:    ], 'o', label='7')\n",
    "    \n",
    "    plt.plot(y[i,0],y[i,1], 'X', label='true', color='purple',  markersize=10)\n",
    "    if res is not None:\n",
    "        res = res.cpu().clone().detach()\n",
    "        plt.title(AMINO_ACID_MAP_INV[res[i].argmax().item()])\n",
    "    if logits is not None:\n",
    "        logits = logits.cpu().clone().detach()\n",
    "        plt.plot(logits[i,0].detach(),logits[i,1].detach(), 'X', label='pred', color='black', markersize=10)\n",
    "    if logits2 is not None:\n",
    "        logits2 = logits2.cpu().clone().detach()\n",
    "        plt.plot(logits2[i,0].detach(),logits2[i,1].detach(), 'X', label='pred2', color='orange', markersize=10)\n",
    "    plt.legend()\n",
    "# plot(X,y,7, res=xres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    if type(model) == nn.DataParallel:\n",
    "        model = model.module\n",
    "    torch.save(model.state_dict(), path)\n",
    "def load_model(model, path):\n",
    "    if type(model) == nn.DataParallel:\n",
    "        model = model.module\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = sum([l for l in lengths])\n",
    "s = [sum(lengths[:i]) for i,l in enumerate(lengths)]\n",
    "device = 'cuda:0'\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h = 32\n",
    "        h = self.h\n",
    "        nl = 1\n",
    "        p_drop = 0.0\n",
    "        mlp_h = 20\n",
    "        self.lstm1 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.lstm2 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.lstm3 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.lstm4 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.ln1 = nn.LayerNorm(h*8, elementwise_affine=False)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(h*8+20, mlp_h)\n",
    "        self.ln2 = nn.LayerNorm(mlp_h, elementwise_affine=False)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(mlp_h, 2)\n",
    "        # self.fc3 = nn.Linear(4, 2)\n",
    "        \n",
    "    def forward(self, x, xres):\n",
    "        x1 = self._sort(x[:,:,s[0]:s[1]].permute(0,2,1))\n",
    "        x2 = self._sort(x[:,:,s[1]:s[2]].permute(0,2,1))\n",
    "        x3 = self._sort(x[:,:,s[2]:s[3]].permute(0,2,1))\n",
    "        x4 = self._sort(x[:,:,s[3]:    ].permute(0,2,1))\n",
    "\n",
    "        # h = self.lstm1(x1)[1][0]  # num_layers * num_directions, batch, hidden_size\n",
    "        # h = h.permute(1,0,2)      # batch, num_layers * num_directions, hidden_size\n",
    "        # h = h[:,-2:,:]            # batch, 2, hidden_size (final hidden state of each direction for last layer)\n",
    "        # x = h.flatten(1)          # batch, 2*hidden_size\n",
    "        x1 = self.lstm1(x1)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x2 = self.lstm2(x2)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x3 = self.lstm3(x3)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x4 = self.lstm4(x4)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x = torch.cat([x1,x2,x3,x4], dim=1)\n",
    "        x = self.ln1(x)\n",
    "        x = torch.cat([x, xres], dim=1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(F.relu(x))\n",
    "        x = self.ln2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(F.relu(x))\n",
    "        # x = self.fc3(F.relu(x))\n",
    "        return x\n",
    "    \n",
    "    # def get_encoding(self, x, xres):\n",
    "    #     x1 = self._sort(x[:,:,s[0]:s[1]].permute(0,2,1))\n",
    "    #     x2 = self._sort(x[:,:,s[1]:s[2]].permute(0,2,1))\n",
    "    #     x3 = self._sort(x[:,:,s[2]:s[3]].permute(0,2,1))\n",
    "    #     x4 = self._sort(x[:,:,s[3]:    ].permute(0,2,1))\n",
    "\n",
    "    #     x1 = self.lstm1(x1)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "    #     x2 = self.lstm2(x2)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "    #     x3 = self.lstm3(x3)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "    #     x4 = self.lstm4(x4)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "    #     x = torch.cat([x1,x2,x3,x4], dim=1)\n",
    "    #     x = torch.cat([x1,x2,x3,x4], dim=1)\n",
    "    #     x = torch.cat([x, xres], dim=1)\n",
    "    #     return x\n",
    "    \n",
    "    def _sort(self, x):\n",
    "        # idxs = x.sum(dim=2).sort()[1].unsqueeze(-1).expand(-1,-1,2)\n",
    "        # x = x.gather(1, idxs)\n",
    "        return x\n",
    "    \n",
    "model = nn.DataParallel(LSTMNet()).to(device)\n",
    "criterion = nn.MSELoss() # try cosine similarity\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,xres,y = next(iter(trainloader))\n",
    "model(X.to(device),xres.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 22/1500 [01:45<1:57:29,  4.77s/it, train_loss=1.15e+3, test_loss=1.47e+3]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "lowest_test_loss = float('inf')\n",
    "writer = SummaryWriter('tensorboard')\n",
    "for epoch in (pbar := tqdm(range(1500))):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for X,xres,y in trainloader:\n",
    "        X,xres,y = X.to(device), xres.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X, xres)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    train_losses.append(sum(losses) / len(losses))\n",
    "\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for X,xres,y in testloader:\n",
    "        with torch.no_grad():\n",
    "            X,xres,y = X.to(device), xres.to(device), y.to(device)\n",
    "            logits = model(X, xres)\n",
    "            loss = criterion(logits, y)\n",
    "            losses.append(loss.item())\n",
    "    test_losses.append(sum(losses) / len(losses))\n",
    "    if test_losses[-1] < lowest_test_loss:\n",
    "        lowest_test_loss = test_losses[-1]\n",
    "        save_model(model, 'ml_data/best_model.pt')\n",
    "    writer.add_scalars('Loss', {\n",
    "        'train': train_losses[-1],\n",
    "        'test': test_losses[-1]\n",
    "    }, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    pbar.set_postfix({'train_loss': train_losses[-1], 'test_loss': test_losses[-1]})\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses[:], label='train')\n",
    "plt.plot(test_losses[:], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,xres,y in testloader:\n",
    "    with torch.no_grad():\n",
    "        X,xres,y = X.to(device), xres.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "    break\n",
    "plot(X, y, 5, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Across Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiProteinDataset(Dataset): # dont use with DataLoader\n",
    "    def __init__(self, protein_ids, batch_size, weights_file, device):\n",
    "        # self.path = path\n",
    "        # self.ids = [f.name.split('.')[0] for f in path.iterdir() if f.is_file()]\n",
    "        self.ids = protein_ids\n",
    "        self.proteinloaders = {id:DataLoader(ProteinDataset(id, path), batch_size=batch_size, shuffle=False) for id in self.ids}\n",
    "        self.pretrained = LSTMNet().to(device)\n",
    "        self.pretrained.eval()\n",
    "        load_model(self.pretrained, weights_file)\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        encs = []\n",
    "        ys = []\n",
    "        with torch.no_grad():\n",
    "            for X,xres,y in self.proteinloaders[self.ids[i]]:\n",
    "                # enc = self.pretrained(X.to(device), xres.to(device))\n",
    "                enc = self.pretrained.get_encoding(X.to(device), xres.to(device))\n",
    "                encs.append(enc)\n",
    "                ys.append(y)\n",
    "        encs = torch.cat(encs)\n",
    "        ys = torch.cat(ys)\n",
    "        return encs.unsqueeze(0), ys\n",
    "    def get_data(self, i):\n",
    "        Xs = []\n",
    "        ys = []\n",
    "        with torch.no_grad():\n",
    "            for X,xres,y in self.proteinloaders[self.ids[i]]:\n",
    "                Xs.append(X)\n",
    "                ys.append(y)\n",
    "        Xs = torch.cat(Xs)\n",
    "        ys = torch.cat(ys)\n",
    "        return Xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "trainset = MultiProteinDataset(train, 512, '/home/musa/protein-dihedral-visualization/ml_data/best_model_xres_h32_nl1_mlp20_dropout30_1.7k.pt', device)\n",
    "testset = MultiProteinDataset(test, 512, '/home/musa/protein-dihedral-visualization/ml_data/best_model_xres_h32_nl1_mlp20_dropout30_1.7k.pt', device)\n",
    "# trainloader = DataLoader(trainset, batch_size=1, shuffle=False)\n",
    "# testloader = DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "X_data,y = testset.get_data(i)\n",
    "X,y = testset[i]\n",
    "plot(X_data, y, 0, X.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9490"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMNet_P2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = 276\n",
    "        self.h = 4\n",
    "        nl = 2\n",
    "        p_drop = 0.0\n",
    "        mlp_h = 8\n",
    "        self.lstm = nn.LSTM(self.input, self.h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.fc = nn.Linear(self.h*2, 2)\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        # self.fc2 = nn.Linear(mlp_h, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h,c) = self.lstm(x) # keep concatenated hidden states from last layer of each direction for all timesteps\n",
    "        x = x.squeeze(0) # turn sequence dim into batch dim (1, L, h) -> (L, h)\n",
    "        x = DataLoader(TensorDataset(x), batch_size=512, shuffle=False) # split into batches in case protein is too long\n",
    "        logits = []\n",
    "        for X in x:\n",
    "            X = self.fc(F.relu(X[0]))\n",
    "            logits.append(X)\n",
    "            # logits.append(self.fc2(F.relu(X)))\n",
    "        return torch.cat(logits)\n",
    "model = nn.DataParallel(LSTMNet_P2()).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) # try lower learning rate\n",
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = trainset[0]\n",
    "model(X.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [55:08<00:00,  2.21s/it, train_loss=946, test_loss=2.59e+3]    \n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "lowest_test_loss = float('inf')\n",
    "writer = SummaryWriter('tensorboard2')\n",
    "for epoch in (pbar := tqdm(range(1500))):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for X,y in trainset:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    train_losses.append(sum(losses) / len(losses))\n",
    "\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for X,y in testset:\n",
    "        with torch.no_grad():\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            losses.append(loss.item())\n",
    "    test_losses.append(sum(losses) / len(losses))\n",
    "    if test_losses[-1] < lowest_test_loss:\n",
    "        lowest_test_loss = test_losses[-1]\n",
    "        save_model(model, 'ml_data/best_model_p2.pt')\n",
    "    writer.add_scalars('Loss', {\n",
    "        'train': train_losses[-1],\n",
    "        'test': test_losses[-1]\n",
    "    }, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    pbar.set_postfix({'train_loss': train_losses[-1], 'test_loss': test_losses[-1]})\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "X_data,y = testset.get_data(i)\n",
    "X,y = testset[i]\n",
    "logits = model(X.to(device))\n",
    "plot(X_data, y, 4, X.squeeze(0), logits.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X.to(device)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDE Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def get_kdepeak(x):\n",
    "    kdews = [1,32,64,128]\n",
    "    Xi = x.cpu().numpy().copy()\n",
    "    kdes = []\n",
    "    for i in range(Xi.shape[0]):\n",
    "        mask = (Xi[i,0] == 0) & (Xi[i,1] == 0)\n",
    "        Xi[i,:,mask] = np.nan\n",
    "        x1 = Xi[i,:,s[0]:s[1]]\n",
    "        x1 = x1[:,~np.isnan(x1).any(axis=0)]\n",
    "        w1 = np.full(x1.shape[1], kdews[0])\n",
    "        x2 = Xi[i,:,s[1]:s[2]]\n",
    "        x2 = x2[:,~np.isnan(x2).any(axis=0)]\n",
    "        w2 = np.full(x2.shape[1], kdews[1])\n",
    "        x3 = Xi[i,:,s[2]:s[3]]\n",
    "        x3 = x3[:,~np.isnan(x3).any(axis=0)]\n",
    "        w3 = np.full(x3.shape[1], kdews[2])\n",
    "        x4 = Xi[i,:,s[3]:]\n",
    "        x4 = x4[:,~np.isnan(x4).any(axis=0)]\n",
    "        w4 = np.full(x4.shape[1], kdews[3])\n",
    "\n",
    "        x = np.concatenate([x1,x2,x3,x4], axis=1)\n",
    "        w = np.concatenate([w1,w2,w3,w4])\n",
    "\n",
    "        if x.shape[1] == 0:\n",
    "            kdes.append(np.full(2, np.nan))\n",
    "            continue\n",
    "        try:\n",
    "            kde = gaussian_kde(x, weights=w)\n",
    "        except:\n",
    "            kdes.append(np.full(2, np.nan))\n",
    "            continue\n",
    "        kdepeak = x[:,np.argmax(kde(x))]\n",
    "        kdes.append(kdepeak)\n",
    "    return np.stack(kdes)\n",
    "\n",
    "# Eucledian distance\n",
    "def diff(x1, x2):\n",
    "    d = np.abs(x1 - x2)\n",
    "    return np.minimum(d, 360-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(kdes, 'ml_data/newdata_kdes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [32:40<00:00, 30.16s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m kdes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(kdes)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# losses = np.mean(losses)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mlosses\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# model = nn.DataParallel(LSTMNet().to(device))\n",
    "# load_model(model, 'ml_data/best_model_newdata_ln_h32_mlp20_do30_1.5k.pt')\n",
    "# model.eval()\n",
    "# preds = []\n",
    "true = []\n",
    "kdes = []\n",
    "# losses = []\n",
    "\n",
    "# model.eval()\n",
    "for X,xres,y in tqdm(testloader):\n",
    "    with torch.no_grad():\n",
    "        # X,xres,y = X.to(device), xres.to(device), y.to(device)\n",
    "        # logits = model(X, xres)\n",
    "        # loss = criterion(logits, y)\n",
    "        # losses.append(loss.item())\n",
    "        # preds.append(logits.cpu().numpy())\n",
    "        true.append(y.cpu().numpy())    \n",
    "        kdes.append(get_kdepeak(X))\n",
    "# preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "kdes = np.concatenate(kdes)\n",
    "# losses = np.mean(losses)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1472.5476388784555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.212742, 37.45628, 963903.6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_dist = np.sqrt(diff(preds[:,0], true[:,0])**2 + diff(preds[:,1], true[:,1])**2)\n",
    "preds_dist.mean(), preds_dist.std(), preds_dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dist = np.sqrt(diff(preds[:,0], true[:,0])**2 + diff(preds[:,1], true[:,1])**2)\n",
    "kdes_dist = np.sqrt(diff(kdes[:,0], true[:,0])**2 + diff(kdes[:,1], true[:,1])**2)\n",
    "preds_dist[np.where(np.isnan(kdes_dist))] = np.nan\n",
    "\n",
    "df = pd.DataFrame({'preds_dist': preds_dist, 'kdes_dist': kdes_dist})\n",
    "print(df.describe())\n",
    "# sns.kdeplot(data=df, x='preds_dist', fill=True, label='preds')\n",
    "# sns.kdeplot(data=df, x='kdes_dist', fill=True, label='kdes')\n",
    "# m = max(df.preds_dist.max(), df.kdes_dist.max())\n",
    "m = np.sqrt(180**2 + 180**2)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,7), sharex=True)\n",
    "\n",
    "kde = gaussian_kde(df.preds_dist.dropna().values, bw_method=0.1)\n",
    "x = np.linspace(0, m, 10000)\n",
    "p = kde(x)\n",
    "c = np.cumsum(p) / np.sum(p)\n",
    "axes[0].plot(x, p, label='Model')\n",
    "axes[0].fill_between(x, 0, p, alpha=0.2)\n",
    "axes[1].plot(x, c, label='Model')\n",
    "axes[1].fill_between(x, 0, c, alpha=0.2)\n",
    "\n",
    "kde = gaussian_kde(df.kdes_dist.dropna().values)\n",
    "x = np.linspace(0, m, 10000)\n",
    "p = kde(x)\n",
    "c = np.cumsum(p) / np.sum(p)\n",
    "axes[0].plot(x, p, label='KDE')\n",
    "axes[0].fill_between(x, 0, p, alpha=0.2)\n",
    "axes[1].plot(x, c, label='KDE')\n",
    "axes[1].fill_between(x, 0, c, alpha=0.2)\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[1].set_ylabel('Cumulative Density')\n",
    "\n",
    "fig.suptitle('Distance From Predicted (Model vs KDE) Phi/Psi to X-ray Phi/Psi')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(LSTMNet().to(device))\n",
    "load_model(model, 'ml_data/best_model_h32_nl1_mlp12_dropout_1.7k.pt')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(testloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X.to(device))\n",
    "kdepeak = get_kdepeak(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(kdes_dist)[-10:]\n",
    "# print(kdes_dist[1075], preds_dist[1075])\n",
    "X,y = testloader.dataset[88]\n",
    "X = X.unsqueeze(0)\n",
    "y = y.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    logits = model(X.to(device))\n",
    "kdepeak = get_kdepeak(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_and_kde(X, y, i, logits, kdepeak):\n",
    "    y = y.numpy().copy()\n",
    "    X = X.numpy().copy()\n",
    "    logits = logits.cpu().numpy().copy()\n",
    "    s = [sum(lengths[:i]) for i,l in enumerate(lengths)]\n",
    "    X[i, :, (X[i,0] == 0) & (X[i,1] == 0)] = np.nan\n",
    "    \n",
    "    sns.set_palette(\"deep\")\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    sns.scatterplot(x=X[i, 0, s[0]:s[1]], y=X[i, 1, s[0]:s[1]], label='Win 4', edgecolor=None, ax=ax, s=20)\n",
    "    sns.scatterplot(x=X[i, 0, s[1]:s[2]], y=X[i, 1, s[1]:s[2]], label='Win 5', edgecolor=None, ax=ax, s=20)\n",
    "    sns.scatterplot(x=X[i, 0, s[2]:s[3]], y=X[i, 1, s[2]:s[3]], label='Win 6', edgecolor=None, ax=ax, s=20)\n",
    "    sns.scatterplot(x=X[i, 0, s[3]:    ], y=X[i, 1, s[3]:    ], label='Win 7', edgecolor=None, ax=ax, s=20)\n",
    "    \n",
    "    sns.set_palette(\"bright\")\n",
    "    sns.scatterplot(x=[y[i,0]], y=[y[i,1]], marker='X', label='X-Ray', s=100, linewidth=1.5, ax=ax)\n",
    "    sns.scatterplot(x=[logits[i,0]], y=[logits[i,1]], marker='X', label='Model Prediction',  s=100, linewidth=1.5, ax=ax)\n",
    "    sns.scatterplot(x=[kdepeak[i,0]], y=[kdepeak[i,1]], marker='X', label='PDBMine KDE Prediction',  s=100, linewidth=1.5, ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Phi')\n",
    "    ax.set_ylabel('Psi')\n",
    "    ax.legend()\n",
    "    ax.set_title('PDBMine KDE Prediction vs Model Prediction vs X-Ray Phi/Psi for one kmer')\n",
    "    plt.tight_layout()\n",
    "# 15,16, 30, 38\n",
    "plot_pred_and_kde(X, y, 0, logits, kdepeak)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
