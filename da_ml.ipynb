{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import DihedralAdherence\n",
    "from lib import PDBMineQuery\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "PDBMINE_URL = os.getenv(\"PDBMINE_URL\")\n",
    "PROJECT_DIR = 'tests'\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = ['T1024', 'T1096', 'T1027', 'T1082', 'T1091', 'T1058', 'T1049', 'T1030', 'T1056', 'T1038', 'T1025', 'T1028']\n",
    "da = DihedralAdherence(proteins[0], [4,5,6,7], PDBMINE_URL, PROJECT_DIR, kdews=[1,32,64,128])\n",
    "seqs = da.xray_phi_psi.seq_ctxt.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [4096, 512, 256, 256]\n",
    "length = sum([l for l in lengths])\n",
    "s = [sum(lengths[:i]) for i,l in enumerate(lengths)]\n",
    "device = 'cuda:0'\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h = 32\n",
    "        h = self.h\n",
    "        nl = 1\n",
    "        p_drop = 0.0\n",
    "        mlp_h = 20\n",
    "        self.lstm1 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.lstm2 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.lstm3 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.lstm4 = nn.LSTM(2, h, nl, batch_first=True, bidirectional=True, dropout=p_drop)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(h*8+20, mlp_h)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(mlp_h, 2)\n",
    "    def forward(self, x, xres):\n",
    "        x1 = x[:,:,s[0]:s[1]].permute(0,2,1)\n",
    "        x2 = x[:,:,s[1]:s[2]].permute(0,2,1)\n",
    "        x3 = x[:,:,s[2]:s[3]].permute(0,2,1)\n",
    "        x4 = x[:,:,s[3]:    ].permute(0,2,1)\n",
    "        x1 = self.lstm1(x1)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x2 = self.lstm2(x2)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x3 = self.lstm3(x3)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x4 = self.lstm4(x4)[1][0].permute(1,0,2)[:,-2:,:].flatten(1)\n",
    "        x = torch.cat([x1,x2,x3,x4], dim=1)\n",
    "        x = torch.cat([x, xres], dim=1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(F.relu(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(F.relu(x))\n",
    "        return x    \n",
    "model = (LSTMNet()).to(device)\n",
    "model.load_state_dict(torch.load('ml_data/best_model_xres_h32_nl1_mlp20_dropout30_1.7k.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(Xp, y, i, logits=None, logits2=None, res=None):\n",
    "    ls = lengths\n",
    "    Xp = Xp.cpu().clone().detach()\n",
    "    y = y\n",
    "    Xp[Xp==0] = np.nan\n",
    "    s = [sum(lengths[:i]) for i,l in enumerate(ls)]\n",
    "    s = [sum(lengths[:i]) for i,l in enumerate(lengths)]\n",
    "    plt.plot(Xp[i, 0, s[0]:s[1]], Xp[i, 1, s[0]:s[1]], 'o', label='4')\n",
    "    plt.plot(Xp[i, 0, s[1]:s[2]], Xp[i, 1, s[1]:s[2]], 'o', label='5')\n",
    "    plt.plot(Xp[i, 0, s[2]:s[3]], Xp[i, 1, s[2]:s[3]], 'o', label='6')\n",
    "    plt.plot(Xp[i, 0, s[3]:    ], Xp[i, 1, s[3]:    ], 'o', label='7')\n",
    "    \n",
    "    plt.plot(y[i,0],y[i,1], 'X', label='true', color='purple',  markersize=10)\n",
    "    if logits is not None:\n",
    "        logits = logits.cpu().clone().detach()\n",
    "        plt.plot(logits[i,0].detach(),logits[i,1].detach(), 'X', label='pred', color='black', markersize=10)\n",
    "    if logits2 is not None:\n",
    "        plt.plot(logits2[i,0],logits2[i,1], 'X', label='pred2', color='orange', markersize=10)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import get_phi_psi_dist, find_kdepeak\n",
    "from lib.constants import AMINO_ACID_MAP\n",
    "i = 4\n",
    "phi_psi = get_phi_psi_dist(da.queries, seqs[i])[0]\n",
    "y = da.xray_phi_psi[da.xray_phi_psi.seq_ctxt == seqs[i]][['phi','psi']].values\n",
    "X = []\n",
    "kde = find_kdepeak(phi_psi, None)[['phi','psi']].values.reshape(1,-1)\n",
    "for weight, l in zip([1,32,64,128], lengths):\n",
    "    phi, psi = phi_psi[phi_psi.weight == weight][['phi','psi']].values.T\n",
    "    if phi.shape[0] < l:\n",
    "        phi = np.pad(phi, ((0,l-phi.shape[0])), mode='constant', constant_values=0)\n",
    "        psi = np.pad(psi, ((0,l-psi.shape[0])), mode='constant', constant_values=0)\n",
    "    else:\n",
    "        phi = np.random.choice(phi, l, replace=False)\n",
    "        psi = np.random.choice(psi, l, replace=False)\n",
    "    X.append(np.stack([phi, psi]))\n",
    "xres = F.one_hot(torch.Tensor([AMINO_ACID_MAP[da.get_center(seqs[i])]]).to(torch.int64), 20).to(device)\n",
    "X = np.hstack(X)\n",
    "X = torch.Tensor(X).unsqueeze(0).to(device)\n",
    "logits = model(X, xres)\n",
    "plot(X, y, 0, logits, kde)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
