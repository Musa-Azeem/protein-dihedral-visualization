{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import DihedralAdherence\n",
    "from lib import PDBMineQuery, MultiWindowQuery\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lib.constants import AMINO_ACID_MAP, AMINO_ACID_MAP_INV\n",
    "from lib.across_window_utils import (\n",
    "    get_phi_psi_dist_window, get_afs_window, get_xrays_window, get_cluster_medoid, find_clusters,\n",
    "    precompute_dists, filter_precomputed_dists, \n",
    ")\n",
    "from collections import defaultdict\n",
    "PDBMINE_URL = os.getenv(\"PDBMINE_URL\")\n",
    "PROJECT_DIR = 'ml_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "def plot(q, seq_ctxt, xrays, afs, clusters, phi_psi_dist, precomputed_dists):\n",
    "    n_cluster_plot = 10\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    xrays = xrays.reshape(2, -1)\n",
    "    afs = afs.reshape(2, -1)\n",
    "    print(pd.Series(clusters).value_counts())\n",
    "\n",
    "    cluster_points = phi_psi_dist.groupby(clusters).count().sort_values('phi_0', ascending=False).index.values\n",
    "    clusters_plot = cluster_points[:n_cluster_plot]\n",
    "    medoids = []\n",
    "    for cluster in cluster_points:\n",
    "        medoid = get_cluster_medoid(phi_psi_dist, precomputed_dists, clusters, cluster)\n",
    "        medoids.append(medoid)\n",
    "    medoids = np.array(medoids)\n",
    "\n",
    "    colors = sns.color_palette('Dark2', n_clusters)\n",
    "    fig, axes = plt.subplots(len(clusters_plot), q.winsize, figsize=(16, min(n_cluster_plot, len(clusters_plot))*4), sharey=True, sharex=True)\n",
    "    if axes.ndim == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    for i,axrow in enumerate(axes):\n",
    "        for j, ax in enumerate(axrow):\n",
    "            cluster_dist = phi_psi_dist[clusters == clusters_plot[i]]\n",
    "\n",
    "            sns.scatterplot(data=phi_psi_dist[clusters != clusters_plot[i]], x=f'phi_{j}', y=f'psi_{j}', ax=ax, label='Other Clusters', color='tab:blue', alpha=0.5)\n",
    "            sns.scatterplot(data=cluster_dist, x=f'phi_{j}', y=f'psi_{j}', ax=ax, label=f'Cluster {clusters_plot[i]}', color=colors[i])\n",
    "            ax.scatter(xrays[0,j], xrays[1,j], color='tab:red', marker='X', label='X-ray', zorder=1000)\n",
    "            ax.scatter(afs[0,j], afs[1,j], color='tab:orange', marker='X', label='AF', zorder=1000)\n",
    "            # ax.scatter(pred[0,j], pred[1,j], color='tab:orange', marker='X', label=pred_id, zorder=1000)\n",
    "            ax.scatter(medoids[i].reshape(2,-1)[0,j], medoids[i].reshape(2,-1)[1,j], color='black', marker='X', label='Cluster Centroid', zorder=1000)\n",
    "\n",
    "            def add_conn(xyA, xyB, color, lw, **kwargs):\n",
    "                con = ConnectionPatch(\n",
    "                    xyA=xyA, \n",
    "                    xyB=xyB, \n",
    "                    coordsA=\"data\", coordsB=\"data\", \n",
    "                    axesA=axrow[j], axesB=axrow[j+1], \n",
    "                    color=color, lw=lw, linestyle='--', alpha=0.5, **kwargs\n",
    "                )\n",
    "                fig.add_artist(con)\n",
    "            if j < q.winsize - 1:\n",
    "                # TODO draw lines for 50 points closest to centroid\n",
    "                for k, row in cluster_dist.sample(min(cluster_dist.shape[0], 50)).iterrows():\n",
    "                    add_conn((row[f'phi_{j}'], row[f'psi_{j}']), (row[f'phi_{j+1}'], row[f'psi_{j+1}']), colors[i], 1)\n",
    "                add_conn((xrays[0,j], xrays[1,j]), (xrays[0,j+1], xrays[1,j+1]), 'tab:red', 5, zorder=100)\n",
    "                add_conn((afs[0,j], afs[1,j]), (afs[0,j+1], afs[1,j+1]), 'tab:orange', 5, zorder=100)\n",
    "                # add_conn((pred[0,j], pred[1,j]), (pred[0,j+1], pred[1,j+1]), 'tab:orange', 5, zorder=100)\n",
    "                add_conn((medoids[i].reshape(2,-1)[0,j], medoids[i].reshape(2,-1)[1,j]), (medoids[i].reshape(2,-1)[0,j+1], medoids[i].reshape(2,-1)[1,j+1]), 'black', 5, zorder=100)\n",
    "\n",
    "            ax.set_xlim(-180, 180)\n",
    "            ax.set_ylim(-180, 180)\n",
    "            ax.set_xlabel('')\n",
    "            if j == q.winsize - 1:\n",
    "                ax.legend()\n",
    "            else:\n",
    "                ax.legend().remove()\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'Cluster {clusters_plot[i]} [{cluster_dist.shape[0]}]')\n",
    "    fig.supxlabel('Phi')\n",
    "    fig.supylabel('Psi')\n",
    "    # fig.suptitle(\n",
    "    #     # f'Clustered Phi/Psi Distributions for {seq_ctxt} in protein {da.casp_protein_id}: N={n_points} Silhouette Score: {sil_score:.2f}, X-ray Score [Cluster {nearest_cluster}]: {xray_sil:.2f}, Prediction Score [Cluster {nearest_cluster_pred}]: {pred_sil:.2f}', \n",
    "    #     f'Clustered Phi/Psi Distributions for {seq_ctxt} in protein {da.casp_protein_id}: N={n_points} ({n_unassigned} unassigned) Silhouette Score: {sil_score:.2f}, X-ray Score [Cluster {nearest_cluster}]: {xray_maha:.2f}', \n",
    "    #     y=1.01\n",
    "    # )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5NUP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "pdb_codes = json.load(open('proteins.json'))\n",
    "ml_data = [f.name.split('_')[0] for f in Path('ml_data').iterdir()]\n",
    "for pdb_code in pdb_codes[::-1]:\n",
    "    if pdb_code in ml_data:\n",
    "        print(pdb_code)\n",
    "        break\n",
    "pdb_codes.index(pdb_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1FTG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_codes = [f.name.split('_')[0] for f in Path(PROJECT_DIR).iterdir() if f.is_dir()]\n",
    "ml_samples = [f.stem for f in Path('ml_samples/medoids').iterdir()]\n",
    "for pdb_code in pdb_codes[::-1]:\n",
    "    if pdb_code in ml_samples:\n",
    "        print(pdb_code)\n",
    "        break\n",
    "pdb_codes.index(pdb_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_codes = [f.name.split('_')[0] for f in Path(PROJECT_DIR).iterdir() if f.is_dir()]\n",
    "winsizes = [4,5,6,7]\n",
    "outdir = Path(f'ml_samples/medoids-v2')\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "X_lens = [5, 3, 2] #[15, 5, 3, 2]\n",
    "\n",
    "for id in pdb_codes:\n",
    "    if (outdir / f'{id}.pt').exists():\n",
    "        print('Skipping', id)\n",
    "        continue\n",
    "    try:\n",
    "        da = MultiWindowQuery(id, winsizes, PDBMINE_URL, PROJECT_DIR)\n",
    "        da.load_results()\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if da.af_phi_psi is None:\n",
    "        continue\n",
    "    print(id)\n",
    "\n",
    "    # SKIP FIRST WINDOW SIZE\n",
    "    da.winsizes = da.winsizes[1:]\n",
    "    da.queries = da.queries[1:]\n",
    "\n",
    "    center_idx_ctxt = da.queries[-1].get_center_idx_pos()\n",
    "    winsize_ctxt = da.queries[-1].winsize\n",
    "    # seqs_for_window = da.seqs[center_idx_ctxt:-(winsize_ctxt - center_idx_ctxt - 1)]\n",
    "    # seqs_for_window = pd.DataFrame({'seq_ctxt': seqs_for_window})\n",
    "    seqs = da.xray_phi_psi[['seq_ctxt', 'pos']]\n",
    "    seqs = seqs[~seqs.seq_ctxt.str.contains('X')]\n",
    "    seqs = seqs.drop_duplicates('seq_ctxt')\n",
    "    seqs = seqs[seqs.seq_ctxt.isin(da.af_phi_psi.seq_ctxt)]\n",
    "    seqs = seqs.rename(columns={'seq_ctxt': 'seq'})\n",
    "\n",
    "    if seqs.shape[0] == 0:\n",
    "        print('No sequences for', id)\n",
    "        continue\n",
    "    print(seqs.shape, seqs.seq.nunique())\n",
    "\n",
    "    x_medoids = defaultdict(list)\n",
    "    x_af = defaultdict(list)\n",
    "    x_res = []\n",
    "    y = []\n",
    "    pos = []\n",
    "    seqs_ = []\n",
    "\n",
    "    for i,row in tqdm(seqs.iterrows()):\n",
    "        # Check if alphafold data is complete for largest window size\n",
    "        afs = get_afs_window(da, da.queries[-1], row.seq)\n",
    "        if (afs is None) or (afs.shape[0] != da.queries[-1].winsize*2) or (np.isnan(afs).any()):\n",
    "            # print(f\"AF data for {row.seq} is incomplete\")\n",
    "            continue\n",
    "        # Check if xrays are complete for largest window size\n",
    "        xrays = get_xrays_window(da, da.queries[-1], row.seq)\n",
    "        if xrays.shape[0] != da.queries[-1].winsize*2 or np.isnan(xrays).any():\n",
    "            # print(f\"Xray data for {row.seq} is incomplete\")\n",
    "            continue\n",
    "        for j,q in enumerate(da.queries):\n",
    "            xrays = get_xrays_window(da, q, row.seq)\n",
    "            afs = get_afs_window(da, q, row.seq)\n",
    "            phi_psi_dist = get_phi_psi_dist_window(q, row.seq)\n",
    "            skip = False\n",
    "\n",
    "            phi_psi_dist = phi_psi_dist.dropna()\n",
    "            phi_psi_dist = phi_psi_dist[(phi_psi_dist <= 180).all(axis=1)]\n",
    "            \n",
    "            if phi_psi_dist.shape[0] == 0:\n",
    "                # print(f\"No pdbmine data for {row.seq}\")\n",
    "                skip = True\n",
    "            if phi_psi_dist.shape[1] != q.winsize*2:\n",
    "                # print(f\"Phi/Psi data for {row.seq} is incomplete\")\n",
    "                skip = True\n",
    "            if phi_psi_dist.shape[0] > 10000:\n",
    "                phi_psi_dist = phi_psi_dist.sample(10000)\n",
    "            \n",
    "            medoids = np.zeros([X_lens[j], q.winsize*2])\n",
    "\n",
    "            if not skip and phi_psi_dist.shape[0] == 1:\n",
    "                medoids[0] = phi_psi_dist.iloc[0].values\n",
    "            elif not skip and phi_psi_dist.shape[0] > 1:\n",
    "                # Cluster\n",
    "                dists = precompute_dists(phi_psi_dist)\n",
    "                n_clusters, clusters = find_clusters(dists, min_cluster_size=np.min([phi_psi_dist.shape[0], 20]), cluster_selection_epsilon=30)\n",
    "\n",
    "                if n_clusters == 0:\n",
    "                    n_clusters, clusters = find_clusters(dists, min_cluster_size=2, cluster_selection_epsilon=60)\n",
    "                \n",
    "                if n_clusters == 0:\n",
    "                    n_clusters, clusters = find_clusters(dists, min_cluster_size=2, cluster_selection_epsilon=120)\n",
    "                \n",
    "                if n_clusters > 0:\n",
    "                    dists, phi_psi_dist, clusters = filter_precomputed_dists(dists, phi_psi_dist, clusters)\n",
    "                    cluster_counts = pd.Series(clusters).value_counts().sort_values(ascending=False)\n",
    "                    for k,cluster in zip(range(X_lens[j]), cluster_counts.index):\n",
    "                        medoid = get_cluster_medoid(phi_psi_dist, dists, clusters, cluster)\n",
    "                        medoids[k] = medoid\n",
    "\n",
    "            x_medoids[j].append(torch.tensor(medoids, dtype=torch.float32))\n",
    "            x_af[j].append(torch.tensor(afs, dtype=torch.float32))\n",
    "        pos.append(row.pos)\n",
    "        seqs_.append(row.seq)\n",
    "        x_res.append(AMINO_ACID_MAP[row.seq[center_idx_ctxt]])\n",
    "        # y.append(torch.tensor(xrays.reshape(2, -1)[:, center_idx_ctxt], dtype=torch.float32))\n",
    "        y.append(torch.tensor(xrays, dtype=torch.float32))\n",
    "        if torch.isnan(y[-1]).any():\n",
    "            print('Xray data is nan for', row.seq)\n",
    "\n",
    "    if len(y) == 0:\n",
    "        print('No data for', id)\n",
    "        continue\n",
    "    for i in range(len(da.queries)):\n",
    "        x_medoids[i] = torch.stack(x_medoids[i])\n",
    "        x_af[i] = torch.stack(x_af[i])\n",
    "\n",
    "    x_res = F.one_hot(torch.tensor(x_res).to(torch.int64), num_classes=20).float()\n",
    "    y = torch.stack(y)\n",
    "    pos = torch.tensor(pos)\n",
    "    torch.save({\n",
    "        'x_medoids': list(x_medoids.values()),\n",
    "        'x_af': list(x_af.values()),\n",
    "        'x_res': x_res,\n",
    "        'pos': pos,\n",
    "        'y': y,\n",
    "        'seq': seqs_\n",
    "    }, outdir / f'{id}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('ml_samples/medoids-v2/1EBA.pt')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(Path('ml_samples/medoids-v2').iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_medoids', 'x_af', 'x_res', 'pos', 'y', 'seq'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('ml_samples/medoids-v2/1EBA.pt').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_medoids, x_af, x_res, pos, y, seq = torch.load('ml_samples/medoids-v2/1EBA.pt').values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 2, 14])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_medoids[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(n_clusters_all[4]).mean(), np.array(n_clusters_all[4]).std()\n",
    "# # np.array(n_clusters_all[7]).mean(), np.array(n_clusters_all[7]).std()\n",
    "# average_clusters = {k: np.array(v).mean() for k,v in n_clusters_all.items()}\n",
    "# std_clusters = {k: np.array(v).std() for k,v in n_clusters_all.items()}\n",
    "\n",
    "# avg:\n",
    "# 4: 12.675570539419088\n",
    "# 5: 2.2909803921568628\n",
    "# 6: 1.2014057853473912\n",
    "# 7: 1.0849134377576257\n",
    "\n",
    "# std:\n",
    "# 4: 7.717139186664446\n",
    "# 5: 1.4648062017775114\n",
    "# 6: 0.5645546946235267\n",
    "# 7: 0.3251658412701237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop: 7->1, 6->1, 5->2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ml(i, x_medoids, x_af, y, pred=None, print_windows=[1,2,3]):\n",
    "    offsets = [2, 1, 1, 0]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,5))\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for k in range(winsizes[-1]):\n",
    "        xs.append(x_af[-1][i, k]+360*k)\n",
    "        ys.append(x_af[-1][i, winsizes[-1]+k])\n",
    "        ax.scatter(xs[-1], ys[-1], color='tab:blue', s=10)\n",
    "    ax.plot(xs, ys, color='tab:blue', label='AF', alpha=0.75)\n",
    "    colors = sns.color_palette('Dark2', sum(X_lens))\n",
    "    for j in range(len(winsizes)):\n",
    "        if j not in print_windows:\n",
    "            continue\n",
    "        for li in range(X_lens[j]):\n",
    "            xs = []\n",
    "            ys = []\n",
    "            if torch.any(x_medoids[j][i, li] != 0.):\n",
    "                for k in range(winsizes[j]):\n",
    "                    xs.append(x_medoids[j][i, li, k]+360*(k+offsets[j]))\n",
    "                    ys.append(x_medoids[j][i, li, winsizes[j]+k])\n",
    "                    ax.scatter(xs[-1], ys[-1], color=colors[j], s=10)\n",
    "                ax.plot(xs, ys, color=colors[j], label=f'Medoid {j+4} {li}', alpha=0.75, linestyle='--')\n",
    "    for j in range(winsizes[-1]):\n",
    "        ax.vlines(j*360-180, -180, 180, color='black', alpha=0.5, linestyles='--')\n",
    "    ax.scatter(y[i,0]+360*(winsizes[-1] // 2), y[i,1], color='tab:red', marker='X', label='X-ray', s=100)\n",
    "    if pred is not None:\n",
    "        ax.scatter(pred[i,0]+360*(winsizes[-1] // 2), pred[i,1], color='tab:orange', marker='X', label='Prediction', s=100)\n",
    "    ax.set_xlim(-180, 360*winsizes[-1] - 180)\n",
    "    ax.set_xticks(np.arange(-180, 360*winsizes[-1], 180))\n",
    "    # ax.set_xticklabels(np.mod(np.arange(-180, 360*winsizes[0], 180), 360))\n",
    "    ax.set_xticklabels(np.mod(np.arange(-180, 360*winsizes[-1], 180), 360))\n",
    "    ax.set_ylim(-180, 180)\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "def save_model(model, path):\n",
    "    if type(model) == nn.DataParallel:\n",
    "        model = model.module\n",
    "    torch.save(model.state_dict(), path)\n",
    "def load_model(model, path):\n",
    "    if type(model) == nn.DataParallel:\n",
    "        model = model.module\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDatasetMedoid(Dataset):\n",
    "    def __init__(self, id, path):\n",
    "        self.id = id\n",
    "        self.path = path\n",
    "\n",
    "        self.x_medoids, self.x_af, self.x_res, self.y = torch.load(self.path / f'{id}.pt')\n",
    "        self.n_windows = len(self.x_medoids)\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            [self.x_medoids[j][i] for j in range(self.n_windows)],\n",
    "            [self.x_af[j][i] for j in range(self.n_windows)],\n",
    "            self.x_res[i], \n",
    "            self.y[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lens = [5, 3, 2]\n",
    "winsizes = [5,6,7]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f.stem for f in Path('ml_samples/medoids').iterdir()]\n",
    "for id in ids:\n",
    "    x_medoids, x_af, x_res, y = torch.load(f'ml_samples/medoids/{id}.pt')\n",
    "    for m in x_medoids:\n",
    "        if torch.isnan(m).any():\n",
    "            print(id, 'has NaNs m')\n",
    "    for af in x_af:\n",
    "        if torch.isnan(af).any():\n",
    "            print(id, 'has NaNs af')\n",
    "    if torch.isnan(x_res).any():\n",
    "        print(id, 'has NaNs res')\n",
    "    if torch.isnan(y).any():\n",
    "        print(id, 'has NaNs y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f.stem for f in Path('ml_samples/medoids').iterdir()][:350]\n",
    "dataset = ConcatDataset([ProteinDatasetMedoid(pdb_id, Path('ml_samples/medoids')) for pdb_id in ids])\n",
    "train, test = train_test_split(list(range(len(dataset))), test_size=0.2)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_medoids, x_af, x_res, y = next(iter(train_dataloader))\n",
    "plot_ml(6, x_medoids, x_af, y, None, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d, nhead):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d, nhead, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        return self.mha(x, x, x)[0]\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d, nhead, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.mha = SelfAttention(d, nhead)\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(d, 2*d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2*d, d),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Net (nn.Module):\n",
    "    def __init__(self, x_lens, winsizes, device):\n",
    "        super().__init__()\n",
    "        self.x_lens = x_lens\n",
    "        self.winsizes = winsizes\n",
    "        self.n_medoids = sum(x_lens[1:])\n",
    "        self.af_input_size = sum([w*2 for w in winsizes])\n",
    "        self.input_size = sum([l*w*2 for l,w in zip(x_lens, winsizes)])\n",
    "        self.device = device\n",
    "\n",
    "        self.d = 32\n",
    "        nheads = 2\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Linear(w*2, self.d, bias=False) for w in winsizes[1:]])\n",
    "        \n",
    "        dropout = 0.15\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(self.d, nheads, dropout),\n",
    "            # Block(self.d, nheads, dropout),\n",
    "            # Block(self.d, nheads, dropout),\n",
    "            # Block(self.d, nheads, dropout)\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(self.d)\n",
    "        # self.out1 = nn.Linear(self.d, 2)\n",
    "        # self.out2 = nn.Linear(2*self.n_medoids, 2)\n",
    "        self.out = nn.Linear(self.d*self.n_medoids, 2)\n",
    "        # self.out = nn.Linear(self.d, 2)\n",
    "\n",
    "    def emb_mask(self, x_medoids):\n",
    "        xs = [[] for _ in range(x_medoids[0].shape[0])] # batch size\n",
    "        for i,x in enumerate(x_medoids[1:]):\n",
    "            mask = torch.any(x, dim=2)\n",
    "            for j,xi in enumerate(x):\n",
    "                xs[j].append(self.embs[i](xi[mask[j]]))\n",
    "        return torch.nested.nested_tensor([torch.cat(xi, dim=0) for xi in xs])\n",
    "\n",
    "    def forward(self, x_medoids, x_af):\n",
    "        # x = torch.cat([self.embs[i](x[:,:1]) for i,x in enumerate(x_medoids[1:])], dim=1)\n",
    "        x = torch.cat([self.embs[i](x) for i,x in enumerate(x_medoids[1:])], dim=1)\n",
    "        x = self.blocks(x)\n",
    "        # x = x.mean(dim=1)\n",
    "        x = x.flatten(1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def get_optimizer(model):\n",
    "        decay_params = [p for n,p in model.named_parameters() if p.dim() >= 2]\n",
    "        no_decay_params = [p for n,p in model.named_parameters() if p.dim() < 2]\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': decay_params, 'weight_decay': 1e-5},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "        ], betas=(0.7, 0.999), lr=1e-4)\n",
    "        return optimizer\n",
    "    \n",
    "class AngleMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x, y, reduce='mean'):\n",
    "        def diff(x1, x2):\n",
    "            d = torch.abs(x1 - x2)\n",
    "            d = torch.minimum(d, 360-d)\n",
    "            return d\n",
    "        if reduce=='sum':\n",
    "            return torch.sum(diff(x, y)**2)\n",
    "        return torch.mean(diff(x, y)**2)\n",
    "\n",
    "model = nn.DataParallel(Net(X_lens, winsizes, device)).to(device)\n",
    "# model = Net(X_lens, winsizes, device).to(device)\n",
    "criterion = AngleMSELoss().to(device)\n",
    "optimizer = Net.get_optimizer(model)\n",
    "torch.compile(model)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_medoids, x_af, x_res, y = next(iter(train_dataloader))\n",
    "x_medoids = [m.to(device) for m in x_medoids]\n",
    "x_af = [af.to(device) for af in x_af]\n",
    "model(x_medoids, x_af).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [[],[]]\n",
    "for epoch in range(200):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for x_medoids, x_af, x_res, y in train_dataloader:\n",
    "        x_medoids = [m.to(device) for m in x_medoids]\n",
    "        x_af = [af.to(device) for af in x_af]        \n",
    "        y = y.to(device)\n",
    "        if torch.isnan(y).any():\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_medoids, x_af)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    train_loss = sum(train_losses) / len(train_dataloader)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        losses[0].append(train_loss)\n",
    "        test_losses = []\n",
    "        model.eval()\n",
    "        for x_medoids, x_af, x_res, y in test_dataloader:\n",
    "            x_medoids = [m.to(device) for m in x_medoids]\n",
    "            x_af = [af.to(device) for af in x_af]\n",
    "            y = y.to(device)\n",
    "            if torch.isnan(y).any():\n",
    "                continue\n",
    "            y_pred = model(x_medoids, x_af)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_losses.append(loss.item())\n",
    "        test_loss = sum(test_losses) / len(test_dataloader)\n",
    "        losses[1].append(test_loss)\n",
    "\n",
    "        print(f'Epoch {epoch} | Train = {train_loss:6.2f}  | Test = {test_loss:4.2f}')\n",
    "    else:\n",
    "        print(f'Epoch {epoch} | Train = {train_loss:6.2f}')\n",
    "\n",
    "plt.plot(losses[0], label='Train')\n",
    "plt.plot(losses[1], label='Test')\n",
    "plt.legend()\n",
    "plt.ylim(0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0], label='Train')\n",
    "plt.plot(losses[1], label='Test')\n",
    "plt.legend()\n",
    "plt.ylim(0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "for x_medoids, x_af, x_res, y in test_dataloader:\n",
    "    y = y.to(device)\n",
    "    if torch.isnan(y).any():\n",
    "        continue\n",
    "    y_pred = model(x_medoids, x_af)\n",
    "    loss = criterion(y_pred, y)\n",
    "    test_losses.append(loss.item())\n",
    "print('Test:', sum(test_losses) / len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_medoids, x_af, x_res, y = next(iter(dataloader))\n",
    "pred = model(x_af).detach()\n",
    "plot_ml(0, x_medoids, x_af, y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbmine_loss = 0\n",
    "af_loss = 0\n",
    "old_model_loss = 0\n",
    "for x_medoids, x_af, x_res, y in train_dataloader:\n",
    "    af_loss += criterion(x_af[-1][:,[3,10]], y, False).item()\n",
    "    pdbmine_loss += criterion(x_medoids[-1][:,0,[3,10]], y, False).item()\n",
    "    old_model_loss += criterion(old_model(x_res, x_af[-1]), y, False).item()\n",
    "print(af_loss / len(train_dataloader))\n",
    "print(pdbmine_loss / len(train_dataloader))\n",
    "\n",
    "pdbmine_loss = 0\n",
    "af_loss = 0\n",
    "for x_medoids, x_af, x_res, y in test_dataloader:\n",
    "    af_loss += criterion(x_af[-1][:,[3,10]], y, False).item()\n",
    "    pdbmine_loss += criterion(x_medoids[-1][:,0,[3,10]], y, False).item()\n",
    "print(af_loss / len(test_dataloader))\n",
    "print(pdbmine_loss / len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = nn.ModuleList([nn.Linear(w*2, 128) for w in winsizes[1:]])\n",
    "# x = torch.cat([embs[i](x) for i,x in enumerate(x_medoids[1:])], dim=1)\n",
    "xs = [[] for _ in range(512)]\n",
    "for i,x in enumerate(x_medoids[1:]):\n",
    "    mask = torch.any(x, dim=2)\n",
    "    for j,xi in enumerate(x):\n",
    "        xs[j].append(embs[i](xi[mask[j]]))\n",
    "x = torch.nested.nested_tensor([torch.cat(xi, dim=0) for xi in xs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
